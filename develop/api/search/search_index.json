{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"A suite of utilities for AWS Lambda functions to ease adopting best practices such as tracing, structured logging, custom metrics, and more. Looking for a quick read through how the core features are used? Check out this detailed blog post with a practical example. Tenets \u00b6 This project separates core utilities that will be available in other runtimes vs general utilities that might not be available across all runtimes. AWS Lambda only . We optimise for AWS Lambda function environments and supported runtimes only. Utilities might work with web frameworks and non-Lambda environments, though they are not officially supported. Eases the adoption of best practices . The main priority of the utilities is to facilitate best practices adoption, as defined in the AWS Well-Architected Serverless Lens; all other functionality is optional. Keep it lean . Additional dependencies are carefully considered for security and ease of maintenance, and prevent negatively impacting startup time. We strive for backwards compatibility . New features and changes should keep backwards compatibility. If a breaking change cannot be avoided, the deprecation and migration process should be clearly defined. We work backwards from the community . We aim to strike a balance of what would work best for 80% of customers. Emerging practices are considered and discussed via Requests for Comment (RFCs) Progressive . Utilities are designed to be incrementally adoptable for customers at any stage of their Serverless journey. They follow language idioms and their community\u2019s common practices. Install \u00b6 Features \u00b6 Utility Description Tracer Decorators and utilities to trace Lambda function handlers, and both synchronous and asynchronous functions Logger Structured logging made easier, and decorator to enrich structured logging with key Lambda context details Metrics Custom Metrics created asynchronously via CloudWatch Embedded Metric Format (EMF) Environment variables \u00b6 Info Explicit parameters take precedence over environment variables. Environment variable Description Utility Default POWERTOOLS_SERVICE_NAME Sets service name used for tracing namespace, metrics dimension and structured logging All \"service_undefined\" POWERTOOLS_METRICS_NAMESPACE Sets namespace used for metrics Metrics None POWERTOOLS_TRACE_DISABLED Explicitly disables tracing Tracing false POWERTOOLS_TRACER_CAPTURE_RESPONSE Captures Lambda or method return as metadata. Tracing true POWERTOOLS_TRACER_CAPTURE_ERROR Captures Lambda or method exception as metadata. Tracing true POWERTOOLS_LOGGER_LOG_EVENT Logs incoming event Logging false POWERTOOLS_LOGGER_SAMPLE_RATE Debug log sampling Logging 0 POWERTOOLS_LOG_DEDUPLICATION_DISABLED Disables log deduplication filter protection to use Pytest Live Log feature Logging false LOG_LEVEL Sets logging level Logging INFO","title":"Homepage"},{"location":"#tenets","text":"This project separates core utilities that will be available in other runtimes vs general utilities that might not be available across all runtimes. AWS Lambda only . We optimise for AWS Lambda function environments and supported runtimes only. Utilities might work with web frameworks and non-Lambda environments, though they are not officially supported. Eases the adoption of best practices . The main priority of the utilities is to facilitate best practices adoption, as defined in the AWS Well-Architected Serverless Lens; all other functionality is optional. Keep it lean . Additional dependencies are carefully considered for security and ease of maintenance, and prevent negatively impacting startup time. We strive for backwards compatibility . New features and changes should keep backwards compatibility. If a breaking change cannot be avoided, the deprecation and migration process should be clearly defined. We work backwards from the community . We aim to strike a balance of what would work best for 80% of customers. Emerging practices are considered and discussed via Requests for Comment (RFCs) Progressive . Utilities are designed to be incrementally adoptable for customers at any stage of their Serverless journey. They follow language idioms and their community\u2019s common practices.","title":"Tenets"},{"location":"#install","text":"","title":"Install"},{"location":"#features","text":"Utility Description Tracer Decorators and utilities to trace Lambda function handlers, and both synchronous and asynchronous functions Logger Structured logging made easier, and decorator to enrich structured logging with key Lambda context details Metrics Custom Metrics created asynchronously via CloudWatch Embedded Metric Format (EMF)","title":"Features"},{"location":"#environment-variables","text":"Info Explicit parameters take precedence over environment variables. Environment variable Description Utility Default POWERTOOLS_SERVICE_NAME Sets service name used for tracing namespace, metrics dimension and structured logging All \"service_undefined\" POWERTOOLS_METRICS_NAMESPACE Sets namespace used for metrics Metrics None POWERTOOLS_TRACE_DISABLED Explicitly disables tracing Tracing false POWERTOOLS_TRACER_CAPTURE_RESPONSE Captures Lambda or method return as metadata. Tracing true POWERTOOLS_TRACER_CAPTURE_ERROR Captures Lambda or method exception as metadata. Tracing true POWERTOOLS_LOGGER_LOG_EVENT Logs incoming event Logging false POWERTOOLS_LOGGER_SAMPLE_RATE Debug log sampling Logging 0 POWERTOOLS_LOG_DEDUPLICATION_DISABLED Disables log deduplication filter protection to use Pytest Live Log feature Logging false LOG_LEVEL Sets logging level Logging INFO","title":"Environment variables"},{"location":"changelog/","text":"Changelog \u00b6 All notable changes to this project will be documented in this file. This project follows Keep a Changelog format for changes and adheres to Semantic Versioning . [Unreleased] \u00b6 0.1.0 \u00b6 Features \u00b6 tracer: beta release ( #91 ) logger: beta release ( #24 ) metrics: beta release ( #25 )","title":"Changelog"},{"location":"changelog/#changelog","text":"All notable changes to this project will be documented in this file. This project follows Keep a Changelog format for changes and adheres to Semantic Versioning .","title":"Changelog"},{"location":"changelog/#unreleased","text":"","title":"[Unreleased]"},{"location":"changelog/#010","text":"","title":"0.1.0"},{"location":"changelog/#features","text":"tracer: beta release ( #91 ) logger: beta release ( #24 ) metrics: beta release ( #25 )","title":"Features"},{"location":"core/logger/","text":"Logger provides an opinionated logger with output structured as JSON. Key features \u00b6 Capture key fields from Lambda context, cold start and structures logging output as JSON Log Lambda event when instructed (disabled by default) Log sampling enables DEBUG log level for a percentage of requests (disabled by default) Append additional keys to structured log at any point in time Getting started \u00b6 Logger requires two settings: Setting Description Environment variable Constructor parameter Logging level Sets how verbose Logger should be (INFO, by default) LOG_LEVEL level Service Sets service key that will be present across all log statements POWERTOOLS_SERVICE_NAME service Example using AWS Serverless Application Model (SAM) template.yaml 1 2 3 4 5 6 7 8 9 Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : Runtime : nodejs14.x Environment : Variables : LOG_LEVEL : INFO POWERTOOLS_SERVICE_NAME : example index.ts 1 2 3 4 import { Logger } from '@aws-lambda-powertools/logger' ; const logger = Logger (); // Sets service via env var // OR const logger = Logger({ service: 'example' }); Standard structured keys \u00b6 Your Logger will include the following keys to your structured logging: Key Example Note level : str INFO Logging level location : str collect.handler:1 Source code location where statement was executed message : Any Collecting payment Unserializable JSON values are casted as str timestamp : str 2021-05-03 10:20:19,650+0200 Timestamp with milliseconds, by default uses local timezone service : str payment Service name defined, by default service_undefined xray_trace_id : str 1-5759e988-bd862e3fe1be46a994272793 When tracing is enabled , it shows X-Ray Trace ID sampling_rate : float 0.1 When enabled, it shows sampling rate in percentage e.g. 10% exception_name : str ValueError When logger.exception is used and there is an exception exception : str Traceback (most recent call last).. When logger.exception is used and there is an exception Capturing Lambda context info \u00b6 You can enrich your structured logs with key Lambda context information via inject_lambda_context . collect.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) @logger . inject_lambda_context def handler ( event , context ): logger . info ( \"Collecting payment\" ) # You can log entire objects too logger . info ({ \"operation\" : \"collect_payment\" , \"charge_id\" : event [ 'charge_id' ] }) ... Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:7\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" }, { \"level\" : \"INFO\" , \"location\" : \"collect.handler:10\" , \"message\" : { \"operation\" : \"collect_payment\" , \"charge_id\" : \"ch_AZFlk2345C0\" }, \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" } When used, this will include the following keys: Key Example cold_start : bool false function_name str example-powertools-HelloWorldFunction-1P1Z6B39FLU73 function_memory_size : int 128 function_arn : str arn:aws:lambda:eu-west-1:012345678910:function:example-powertools-HelloWorldFunction-1P1Z6B39FLU73 function_request_id : str 899856cb-83d1-40d7-8611-9e78f15f32f4 Logging incoming event \u00b6 When debugging in non-production environments, you can instruct Logger to log the incoming event with log_event param or via POWERTOOLS_LOGGER_LOG_EVENT env var. Warning This is disabled by default to prevent sensitive info being logged. log_handler_event.py 1 2 3 4 5 6 7 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) @logger . inject_lambda_context ( log_event = True ) def handler ( event , context ): ... Setting a Correlation ID \u00b6 You can set a Correlation ID using correlation_id_path param by passing a JMESPath expression . You can retrieve correlation IDs via get_correlation_id method collect.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) @logger . inject_lambda_context ( correlation_id_path = \"headers.my_request_id_header\" ) def handler ( event , context ): logger . debug ( f \"Correlation ID => { logger . get_correlation_id () } \" ) logger . info ( \"Collecting payment\" ) Example Event 1 2 3 4 5 { \"headers\" : { \"my_request_id_header\" : \"correlation_id_value\" } } Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:7\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"correlation_id\" : \"correlation_id_value\" } We provide built-in JMESPath expressions for known event sources, where either a request ID or X-Ray Trace ID are present. collect.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Logger from aws_lambda_powertools.logging import correlation_paths logger = Logger ( service = \"payment\" ) @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST ) def handler ( event , context ): logger . debug ( f \"Correlation ID => { logger . get_correlation_id () } \" ) logger . info ( \"Collecting payment\" ) Example Event 1 2 3 4 5 { \"requestContext\" : { \"requestId\" : \"correlation_id_value\" } } Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:8\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"correlation_id\" : \"correlation_id_value\" } Appending additional keys \u00b6 Custom keys are persisted across warm invocations Always set additional keys as part of your handler to ensure they have the latest value, or explicitly clear them with clear_state=True . You can append additional keys using either mechanism: Persist new keys across all future log messages via append_keys method Add additional keys on a per log message basis via extra parameter append_keys method \u00b6 NOTE: append_keys replaces structure_logs(append=True, **kwargs) method. Both will continue to work until the next major version. You can append your own keys to your existing Logger via append_keys(**additional_key_values) method. collect.py 1 2 3 4 5 6 7 8 9 10 11 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) def handler ( event , context ): order_id = event . get ( \"order_id\" ) # this will ensure order_id key always has the latest value before logging logger . append_keys ( order_id = order_id ) logger . info ( \"Collecting payment\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:11\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"order_id\" : \"order_id_value\" } Logger will automatically reject any key with a None value If you conditionally add keys depending on the payload, you can follow the example above. This example will add order_id if its value is not empty, and in subsequent invocations where order_id might not be present it'll remove it from the Logger. extra parameter \u00b6 Extra parameter is available for all log levels' methods, as implemented in the standard logging library - e.g. logger.info, logger.warning . It accepts any dictionary, and all keyword arguments will be added as part of the root structure of the logs for that log statement. Any keyword argument added using extra will not be persisted for subsequent messages. extra_parameter.py 1 2 3 4 5 6 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) fields = { \"request_id\" : \"1123\" } logger . info ( \"Collecting payment\" , extra = fields ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:6\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"request_id\" : \"1123\" } set_correlation_id method \u00b6 You can set a correlation_id to your existing Logger via set_correlation_id(value) method by passing any string value. collect.py 1 2 3 4 5 6 7 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) def handler ( event , context ): logger . set_correlation_id ( event [ \"requestContext\" ][ \"requestId\" ]) logger . info ( \"Collecting payment\" ) Example Event 1 2 3 4 5 { \"requestContext\" : { \"requestId\" : \"correlation_id_value\" } } Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:7\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"correlation_id\" : \"correlation_id_value\" } Alternatively, you can combine Data Classes utility with Logger to use dot notation object: collect.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Logger from aws_lambda_powertools.utilities.data_classes import APIGatewayProxyEvent logger = Logger ( service = \"payment\" ) def handler ( event , context ): event = APIGatewayProxyEvent ( event ) logger . set_correlation_id ( event . request_context . request_id ) logger . info ( \"Collecting payment\" ) Example Event 1 2 3 4 5 { \"requestContext\" : { \"requestId\" : \"correlation_id_value\" } } Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 { \"timestamp\" : \"2020-05-24 18:17:33,774\" , \"level\" : \"INFO\" , \"location\" : \"collect.handler:9\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"correlation_id\" : \"correlation_id_value\" , \"message\" : \"Collecting payment\" } Removing additional keys \u00b6 You can remove any additional key from Logger state using remove_keys . collect.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) def handler ( event , context ): logger . append_keys ( sample_key = \"value\" ) logger . info ( \"Collecting payment\" ) logger . remove_keys ([ \"sample_key\" ]) logger . info ( \"Collecting payment without sample key\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:7\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"sample_key\" : \"value\" }, { \"level\" : \"INFO\" , \"location\" : \"collect.handler:10\" , \"message\" : \"Collecting payment without sample key\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" } Clearing all state \u00b6 Logger is commonly initialized in the global scope. Due to Lambda Execution Context reuse , this means that custom keys can be persisted across invocations. If you want all custom keys to be deleted, you can use clear_state=True param in inject_lambda_context decorator. Info This is useful when you add multiple custom keys conditionally, instead of setting a default None value if not present. Any key with None value is automatically removed by Logger. This can have unintended side effects if you use Layers Lambda Layers code is imported before the Lambda handler. This means that clear_state=True will instruct Logger to remove any keys previously added before Lambda handler execution proceeds. You can either avoid running any code as part of Lambda Layers global scope, or override keys with their latest value as part of handler's execution. collect.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) @logger . inject_lambda_context ( clear_state = True ) def handler ( event , context ): if event . get ( \"special_key\" ): # Should only be available in the first request log # as the second request doesn't contain `special_key` logger . append_keys ( debugging_key = \"value\" ) logger . info ( \"Collecting payment\" ) #1 request 1 2 3 4 5 6 7 8 9 10 11 12 13 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:10\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"special_key\" : \"debug_key\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" } #2 request 1 2 3 4 5 6 7 8 9 10 11 12 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:10\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : false , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" } Logging exceptions \u00b6 Use logger.exception method to log contextual information about exceptions. Logger will include exception_name and exception keys to aid troubleshooting and error enumeration. Tip You can use your preferred Log Analytics tool to enumerate and visualize exceptions across all your services using exception_name key. collect.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) try : raise ValueError ( \"something went wrong\" ) except Exception : logger . exception ( \"Received an exception\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 { \"level\" : \"ERROR\" , \"location\" : \"collect.handler:5\" , \"message\" : \"Received an exception\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"exception_name\" : \"ValueError\" , \"exception\" : \"Traceback (most recent call last):\\n File \\\"<input>\\\", line 2, in <module>\\nValueError: something went wrong\" } Advanced \u00b6 Reusing Logger across your code \u00b6 Logger supports inheritance via child parameter. This allows you to create multiple Loggers across your code base, and propagate changes such as new keys to all Loggers. collect.py 1 2 3 4 5 6 7 8 import shared # Creates a child logger named \"payment.shared\" from aws_lambda_powertools import Logger logger = Logger () # POWERTOOLS_SERVICE_NAME: \"payment\" def handler ( event , context ): shared . inject_payment_id ( event ) ... shared.py 1 2 3 4 5 6 from aws_lambda_powertools import Logger logger = Logger ( child = True ) # POWERTOOLS_SERVICE_NAME: \"payment\" def inject_payment_id ( event ): logger . structure_logs ( append = True , payment_id = event . get ( \"payment_id\" )) In this example, Logger will create a parent logger named payment and a child logger named payment.shared . Changes in either parent or child logger will be propagated bi-directionally. Child loggers will be named after the following convention {service}.{filename} If you forget to use child param but the service name is the same of the parent, we will return the existing parent Logger instead. Sampling debug logs \u00b6 Use sampling when you want to dynamically change your log level to DEBUG based on a percentage of your concurrent/cold start invocations . You can use values ranging from 0.0 to 1 (100%) when setting POWERTOOLS_LOGGER_SAMPLE_RATE env var or sample_rate parameter in Logger. When is this useful? Let's imagine a sudden spike increase in concurrency triggered a transient issue downstream. When looking into the logs you might not have enough information, and while you can adjust log levels it might not happen again. This feature takes into account transient issues where additional debugging information can be useful. Sampling decision happens at the Logger initialization. This means sampling may happen significantly more or less than depending on your traffic patterns, for example a steady low number of invocations and thus few cold starts. Note If you want Logger to calculate sampling upon every invocation, please open a feature request . collect.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Logger # Sample 10% of debug logs e.g. 0.1 logger = Logger ( service = \"payment\" , sample_rate = 0.1 ) def handler ( event , context ): logger . debug ( \"Verifying whether order_id is present\" ) logger . info ( \"Collecting payment\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \"level\" : \"DEBUG\" , \"location\" : \"collect.handler:7\" , \"message\" : \"Verifying whether order_id is present\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"sampling_rate\" : 0.1 }, { \"level\" : \"INFO\" , \"location\" : \"collect.handler:7\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"sampling_rate\" : 0.1 } LambdaPowertoolsFormatter \u00b6 Logger propagates a few formatting configurations to the built-in LambdaPowertoolsFormatter logging formatter. If you prefer configuring it separately, or you'd want to bring this JSON Formatter to another application, these are the supported settings: Parameter Description Default json_serializer function to serialize obj to a JSON formatted str json.dumps json_deserializer function to deserialize str , bytes , bytearray containing a JSON document to a Python obj json.loads json_default function to coerce unserializable values, when no custom serializer/deserializer is set str datefmt string directives (strftime) to format log timestamp %Y-%m-%d %H:%M:%S,%F%z , where %F is a custom ms directive utc set logging timestamp to UTC False log_record_order set order of log keys when logging [\"level\", \"location\", \"message\", \"timestamp\"] kwargs key-value to be included in log messages None LambdaPowertoolsFormatter.py 1 2 3 4 5 from aws_lambda_powertools import Logger from aws_lambda_powertools.logging.formatter import LambdaPowertoolsFormatter formatter = LambdaPowertoolsFormatter ( utc = True , log_record_order = [ \"message\" ]) logger = Logger ( service = \"example\" , logger_formatter = formatter ) Migrating from other Loggers \u00b6 If you're migrating from other Loggers, there are few key points to be aware of: Service parameter , Inheriting Loggers , Overriding Log records , and Logging exceptions . The service parameter \u00b6 Service is what defines the Logger name, including what the Lambda function is responsible for, or part of (e.g payment service). For Logger, the service is the logging key customers can use to search log operations for one or more functions - For example, search for all errors, or messages like X, where service is payment . Inheriting Loggers \u00b6 Python Logging hierarchy happens via the dot notation: service , service.child , service.child_2 For inheritance, Logger uses a child=True parameter along with service being the same value across Loggers. For child Loggers, we introspect the name of your module where Logger(child=True, service=\"name\") is called, and we name your Logger as {service}.{filename} . Danger A common issue when migrating from other Loggers is that service might be defined in the parent Logger (no child param), and not defined in the child Logger: incorrect_logger_inheritance.py 1 2 3 4 5 6 7 8 9 10 import my_module from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) ... # my_module.py from aws_lambda_powertools import Logger logger = Logger ( child = True ) correct_logger_inheritance.py 1 2 3 4 5 6 7 8 9 10 import my_module from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) ... # my_module.py from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" , child = True ) In this case, Logger will register a Logger named payment , and a Logger named service_undefined . The latter isn't inheriting from the parent, and will have no handler, resulting in no message being logged to standard output. Tip This can be fixed by either ensuring both has the service value as payment , or simply use the environment variable POWERTOOLS_SERVICE_NAME to ensure service value will be the same across all Loggers when not explicitly set. Overriding Log records \u00b6 You might want to continue to use the same date formatting style, or override location to display the package.function_name:line_number as you previously had. Logger allows you to either change the format or suppress the following keys altogether at the initialization: location , timestamp , level , xray_trace_id . lambda_handler.py We honour standard logging library string formats . 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools import Logger date_format = \"%m/ %d /%Y %I:%M:%S %p\" location_format = \"[ %(funcName)s ] %(module)s \" # override location and timestamp format logger = Logger ( service = \"payment\" , location = location_format , datefmt = date_format ) # suppress the location key with a None value logger_two = Logger ( service = \"payment\" , location = None ) logger . info ( \"Collecting payment\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 { \"level\" : \"INFO\" , \"location\" : \"[<module>] lambda_handler\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"02/09/2021 09:25:17 AM\" , \"service\" : \"payment\" } Reordering log keys position \u00b6 You can change the order of standard Logger keys or any keys that will be appended later at runtime via the log_record_order parameter. lambda_handler.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools import Logger # make message as the first key logger = Logger ( service = \"payment\" , log_record_order = [ \"message\" ]) # make request_id that will be added later as the first key # Logger(service=\"payment\", log_record_order=[\"request_id\"]) # Default key sorting order when omit # Logger(service=\"payment\", log_record_order=[\"level\",\"location\",\"message\",\"timestamp\"]) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 { \"message\" : \"hello world\" , \"level\" : \"INFO\" , \"location\" : \"[<module>]:6\" , \"timestamp\" : \"2021-02-09 09:36:12,280\" , \"service\" : \"service_undefined\" , \"sampling_rate\" : 0.0 } Setting timestamp to UTC \u00b6 By default, this Logger and standard logging library emits records using local time timestamp. You can override this behaviour via utc parameter: app.py 1 2 3 4 5 6 7 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) logger . info ( \"Local time\" ) logger_in_utc = Logger ( service = \"payment\" , utc = True ) logger_in_utc . info ( \"GMT time zone\" ) Custom function for unserializable values \u00b6 By default, Logger uses str to handle values non-serializable by JSON. You can override this behaviour via json_default parameter by passing a Callable: collect.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools import Logger def custom_json_default ( value ): return f \"<non-serializable: { type ( value ) . __name__ } >\" class Unserializable : pass logger = Logger ( service = \"payment\" , json_default = custom_json_default ) def handler ( event , context ): logger . info ( Unserializable ()) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:8\" , \"message\" : \"\" < n o n - serializable : U nser ializable> \"\" , \"timestamp\" : \"2021-05-03 15:17:23,632+0200\" , \"service\" : \"payment\" } Bring your own handler \u00b6 By default, Logger uses StreamHandler and logs to standard output. You can override this behaviour via logger_handler parameter: collect.py 1 2 3 4 5 6 7 8 9 10 import logging from pathlib import Path from aws_lambda_powertools import Logger log_file = Path ( \"/tmp/log.json\" ) log_file_handler = logging . FileHandler ( filename = log_file ) logger = Logger ( service = \"payment\" , logger_handler = log_file_handler ) logger . info ( \"Collecting payment\" ) Bring your own formatter \u00b6 By default, Logger uses LambdaPowertoolsFormatter that persists its custom structure between non-cold start invocations. There could be scenarios where the existing feature set isn't sufficient to your formatting needs. For minor changes like remapping keys after all log record processing has completed, you can override serialize method from LambdaPowertoolsFormatter : custom_formatter.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools import Logger from aws_lambda_powertools.logging.formatter import LambdaPowertoolsFormatter from typing import Dict class CustomFormatter ( LambdaPowertoolsFormatter ): def serialize ( self , log : Dict ) -> str : \"\"\"Serialize final structured log dict to JSON str\"\"\" log [ \"event\" ] = log . pop ( \"message\" ) # rename message key to event return self . json_serializer ( log ) # use configured json serializer my_formatter = CustomFormatter () logger = Logger ( service = \"example\" , logger_formatter = my_formatter ) logger . info ( \"hello\" ) For replacing the formatter entirely , you can subclass BasePowertoolsFormatter , implement append_keys method, and override format standard logging method. This ensures the current feature set of Logger like injecting Lambda context and sampling will continue to work. Info You might need to implement remove_keys method if you make use of the feature too. collect.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from aws_lambda_powertools import Logger from aws_lambda_powertools.logging.formatter import BasePowertoolsFormatter class CustomFormatter ( BasePowertoolsFormatter ): custom_format = {} # arbitrary dict to hold our structured keys def append_keys ( self , ** additional_keys ): # also used by `inject_lambda_context` decorator self . custom_format . update ( additional_keys ) # Optional unless you make use of this Logger feature def remove_keys ( self , keys : Iterable [ str ]): for key in keys : self . custom_format . pop ( key , None ) def format ( self , record : logging . LogRecord ) -> str : # noqa: A003 \"\"\"Format logging record as structured JSON str\"\"\" return json . dumps ( { \"event\" : super () . format ( record ), \"timestamp\" : self . formatTime ( record ), \"my_default_key\" : \"test\" , ** self . custom_format , } ) logger = Logger ( service = \"payment\" , logger_formatter = CustomFormatter ()) @logger . inject_lambda_context def handler ( event , context ): logger . info ( \"Collecting payment\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 { \"event\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494\" , \"my_default_key\" : \"test\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" } Bring your own JSON serializer \u00b6 By default, Logger uses json.dumps and json.loads as serializer and deserializer respectively. There could be scenarios where you are making use of alternative JSON libraries like orjson . As parameters don't always translate well between them, you can pass any callable that receives a Dict and return a str : collect.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import orjson from aws_lambda_powertools import Logger custom_serializer = orjson . dumps custom_deserializer = orjson . loads logger = Logger ( service = \"payment\" , json_serializer = custom_serializer , json_deserializer = custom_deserializer ) # when using parameters, you can pass a partial # custom_serializer=functools.partial(orjson.dumps, option=orjson.OPT_SERIALIZE_NUMPY) Built-in Correlation ID expressions \u00b6 You can use any of the following built-in JMESPath expressions as part of inject_lambda_context decorator . Escaping necessary for the - character Any object key named with - must be escaped, for example request.headers.\"x-amzn-trace-id\" . Name Expression Description API_GATEWAY_REST \"requestContext.requestId\" API Gateway REST API request ID API_GATEWAY_HTTP \"requestContext.requestId\" API Gateway HTTP API request ID APPSYNC_RESOLVER 'request.headers.\"x-amzn-trace-id\"' AppSync X-Ray Trace ID APPLICATION_LOAD_BALANCER 'headers.\"x-amzn-trace-id\"' ALB X-Ray Trace ID EVENT_BRIDGE \"id\" EventBridge Event ID Testing your code \u00b6 Inject Lambda Context \u00b6 When unit testing your code that makes use of inject_lambda_context decorator, you need to pass a dummy Lambda Context, or else Logger will fail. This is a Pytest sample that provides the minimum information necessary for Logger to succeed: fake_lambda_context_for_logger.py Note that dataclasses are available in Python 3.7+ only. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from dataclasses import dataclass import pytest @pytest . fixture def lambda_context (): @dataclass class LambdaContext : function_name : str = \"test\" memory_limit_in_mb : int = 128 invoked_function_arn : str = \"arn:aws:lambda:eu-west-1:809313241:function:test\" aws_request_id : str = \"52fdfc07-2182-154f-163f-5f0f9a621d72\" return LambdaContext () def test_lambda_handler ( lambda_context ): test_event = { 'test' : 'event' } your_lambda_handler ( test_event , lambda_context ) # this will now have a Context object populated fake_lambda_context_for_logger_py36.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from collections import namedtuple import pytest @pytest . fixture def lambda_context (): lambda_context = { \"function_name\" : \"test\" , \"memory_limit_in_mb\" : 128 , \"invoked_function_arn\" : \"arn:aws:lambda:eu-west-1:809313241:function:test\" , \"aws_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , } return namedtuple ( \"LambdaContext\" , lambda_context . keys ())( * lambda_context . values ()) def test_lambda_handler ( lambda_context ): test_event = { 'test' : 'event' } # this will now have a Context object populated your_lambda_handler ( test_event , lambda_context ) Tip If you're using pytest and are looking to assert plain log messages, do check out the built-in caplog fixture . Pytest live log feature \u00b6 Pytest Live Log feature duplicates emitted log messages in order to style log statements according to their levels, for this to work use POWERTOOLS_LOG_DEDUPLICATION_DISABLED env var. shell 1 POWERTOOLS_LOG_DEDUPLICATION_DISABLED = \"1\" pytest -o log_cli = 1 Warning This feature should be used with care, as it explicitly disables our ability to filter propagated messages to the root logger (if configured). FAQ \u00b6 How can I enable boto3 and botocore library logging? You can enable the botocore and boto3 logs by using the set_stream_logger method, this method will add a stream handler for the given name and level to the logging module. By default, this logs all boto3 messages to stdout. log_botocore_and_boto3.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from typing import Dict , List from aws_lambda_powertools.utilities.typing import LambdaContext from aws_lambda_powertools import Logger import boto3 boto3 . set_stream_logger () boto3 . set_stream_logger ( 'botocore' ) logger = Logger () client = boto3 . client ( 's3' ) def handler ( event : Dict , context : LambdaContext ) -> List : response = client . list_buckets () return response . get ( \"Buckets\" , []) What's the difference between append_keys and extra ? Keys added with append_keys will persist across multiple log messages while keys added via extra will only be available in a given log message operation. Here's an example where we persist payment_id not request_id . Note that payment_id remains in both log messages while booking_id is only available in the first message. lambda_handler.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) def handler ( event , context ): logger . append_keys ( payment_id = \"123456789\" ) try : booking_id = book_flight () logger . info ( \"Flight booked successfully\" , extra = { \"booking_id\" : booking_id }) except BookingReservationError : ... logger . info ( \"goodbye\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"level\" : \"INFO\" , \"location\" : \"<module>:10\" , \"message\" : \"Flight booked successfully\" , \"timestamp\" : \"2021-01-12 14:09:10,859\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"payment_id\" : \"123456789\" , \"booking_id\" : \"75edbad0-0857-4fc9-b547-6180e2f7959b\" }, { \"level\" : \"INFO\" , \"location\" : \"<module>:14\" , \"message\" : \"goodbye\" , \"timestamp\" : \"2021-01-12 14:09:10,860\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"payment_id\" : \"123456789\" } How do I aggregate and search Powertools logs across accounts? As of now, ElasticSearch (ELK) or 3rd party solutions are best suited to this task. Please see this discussion for more information: https://github.com/awslabs/aws-lambda-powertools-python/issues/460","title":"Logger"},{"location":"core/logger/#key-features","text":"Capture key fields from Lambda context, cold start and structures logging output as JSON Log Lambda event when instructed (disabled by default) Log sampling enables DEBUG log level for a percentage of requests (disabled by default) Append additional keys to structured log at any point in time","title":"Key features"},{"location":"core/logger/#getting-started","text":"Logger requires two settings: Setting Description Environment variable Constructor parameter Logging level Sets how verbose Logger should be (INFO, by default) LOG_LEVEL level Service Sets service key that will be present across all log statements POWERTOOLS_SERVICE_NAME service Example using AWS Serverless Application Model (SAM) template.yaml 1 2 3 4 5 6 7 8 9 Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : Runtime : nodejs14.x Environment : Variables : LOG_LEVEL : INFO POWERTOOLS_SERVICE_NAME : example index.ts 1 2 3 4 import { Logger } from '@aws-lambda-powertools/logger' ; const logger = Logger (); // Sets service via env var // OR const logger = Logger({ service: 'example' });","title":"Getting started"},{"location":"core/logger/#standard-structured-keys","text":"Your Logger will include the following keys to your structured logging: Key Example Note level : str INFO Logging level location : str collect.handler:1 Source code location where statement was executed message : Any Collecting payment Unserializable JSON values are casted as str timestamp : str 2021-05-03 10:20:19,650+0200 Timestamp with milliseconds, by default uses local timezone service : str payment Service name defined, by default service_undefined xray_trace_id : str 1-5759e988-bd862e3fe1be46a994272793 When tracing is enabled , it shows X-Ray Trace ID sampling_rate : float 0.1 When enabled, it shows sampling rate in percentage e.g. 10% exception_name : str ValueError When logger.exception is used and there is an exception exception : str Traceback (most recent call last).. When logger.exception is used and there is an exception","title":"Standard structured keys"},{"location":"core/logger/#capturing-lambda-context-info","text":"You can enrich your structured logs with key Lambda context information via inject_lambda_context . collect.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) @logger . inject_lambda_context def handler ( event , context ): logger . info ( \"Collecting payment\" ) # You can log entire objects too logger . info ({ \"operation\" : \"collect_payment\" , \"charge_id\" : event [ 'charge_id' ] }) ... Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:7\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" }, { \"level\" : \"INFO\" , \"location\" : \"collect.handler:10\" , \"message\" : { \"operation\" : \"collect_payment\" , \"charge_id\" : \"ch_AZFlk2345C0\" }, \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" } When used, this will include the following keys: Key Example cold_start : bool false function_name str example-powertools-HelloWorldFunction-1P1Z6B39FLU73 function_memory_size : int 128 function_arn : str arn:aws:lambda:eu-west-1:012345678910:function:example-powertools-HelloWorldFunction-1P1Z6B39FLU73 function_request_id : str 899856cb-83d1-40d7-8611-9e78f15f32f4","title":"Capturing Lambda context info"},{"location":"core/logger/#logging-incoming-event","text":"When debugging in non-production environments, you can instruct Logger to log the incoming event with log_event param or via POWERTOOLS_LOGGER_LOG_EVENT env var. Warning This is disabled by default to prevent sensitive info being logged. log_handler_event.py 1 2 3 4 5 6 7 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) @logger . inject_lambda_context ( log_event = True ) def handler ( event , context ): ...","title":"Logging incoming event"},{"location":"core/logger/#setting-a-correlation-id","text":"You can set a Correlation ID using correlation_id_path param by passing a JMESPath expression . You can retrieve correlation IDs via get_correlation_id method collect.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) @logger . inject_lambda_context ( correlation_id_path = \"headers.my_request_id_header\" ) def handler ( event , context ): logger . debug ( f \"Correlation ID => { logger . get_correlation_id () } \" ) logger . info ( \"Collecting payment\" ) Example Event 1 2 3 4 5 { \"headers\" : { \"my_request_id_header\" : \"correlation_id_value\" } } Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:7\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"correlation_id\" : \"correlation_id_value\" } We provide built-in JMESPath expressions for known event sources, where either a request ID or X-Ray Trace ID are present. collect.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Logger from aws_lambda_powertools.logging import correlation_paths logger = Logger ( service = \"payment\" ) @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST ) def handler ( event , context ): logger . debug ( f \"Correlation ID => { logger . get_correlation_id () } \" ) logger . info ( \"Collecting payment\" ) Example Event 1 2 3 4 5 { \"requestContext\" : { \"requestId\" : \"correlation_id_value\" } } Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:8\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"correlation_id\" : \"correlation_id_value\" }","title":"Setting a Correlation ID"},{"location":"core/logger/#appending-additional-keys","text":"Custom keys are persisted across warm invocations Always set additional keys as part of your handler to ensure they have the latest value, or explicitly clear them with clear_state=True . You can append additional keys using either mechanism: Persist new keys across all future log messages via append_keys method Add additional keys on a per log message basis via extra parameter","title":"Appending additional keys"},{"location":"core/logger/#append_keys-method","text":"NOTE: append_keys replaces structure_logs(append=True, **kwargs) method. Both will continue to work until the next major version. You can append your own keys to your existing Logger via append_keys(**additional_key_values) method. collect.py 1 2 3 4 5 6 7 8 9 10 11 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) def handler ( event , context ): order_id = event . get ( \"order_id\" ) # this will ensure order_id key always has the latest value before logging logger . append_keys ( order_id = order_id ) logger . info ( \"Collecting payment\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:11\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"order_id\" : \"order_id_value\" } Logger will automatically reject any key with a None value If you conditionally add keys depending on the payload, you can follow the example above. This example will add order_id if its value is not empty, and in subsequent invocations where order_id might not be present it'll remove it from the Logger.","title":"append_keys method"},{"location":"core/logger/#extra-parameter","text":"Extra parameter is available for all log levels' methods, as implemented in the standard logging library - e.g. logger.info, logger.warning . It accepts any dictionary, and all keyword arguments will be added as part of the root structure of the logs for that log statement. Any keyword argument added using extra will not be persisted for subsequent messages. extra_parameter.py 1 2 3 4 5 6 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) fields = { \"request_id\" : \"1123\" } logger . info ( \"Collecting payment\" , extra = fields ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:6\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"request_id\" : \"1123\" }","title":"extra parameter"},{"location":"core/logger/#set_correlation_id-method","text":"You can set a correlation_id to your existing Logger via set_correlation_id(value) method by passing any string value. collect.py 1 2 3 4 5 6 7 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) def handler ( event , context ): logger . set_correlation_id ( event [ \"requestContext\" ][ \"requestId\" ]) logger . info ( \"Collecting payment\" ) Example Event 1 2 3 4 5 { \"requestContext\" : { \"requestId\" : \"correlation_id_value\" } } Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:7\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"correlation_id\" : \"correlation_id_value\" } Alternatively, you can combine Data Classes utility with Logger to use dot notation object: collect.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Logger from aws_lambda_powertools.utilities.data_classes import APIGatewayProxyEvent logger = Logger ( service = \"payment\" ) def handler ( event , context ): event = APIGatewayProxyEvent ( event ) logger . set_correlation_id ( event . request_context . request_id ) logger . info ( \"Collecting payment\" ) Example Event 1 2 3 4 5 { \"requestContext\" : { \"requestId\" : \"correlation_id_value\" } } Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 { \"timestamp\" : \"2020-05-24 18:17:33,774\" , \"level\" : \"INFO\" , \"location\" : \"collect.handler:9\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"correlation_id\" : \"correlation_id_value\" , \"message\" : \"Collecting payment\" }","title":"set_correlation_id method"},{"location":"core/logger/#removing-additional-keys","text":"You can remove any additional key from Logger state using remove_keys . collect.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) def handler ( event , context ): logger . append_keys ( sample_key = \"value\" ) logger . info ( \"Collecting payment\" ) logger . remove_keys ([ \"sample_key\" ]) logger . info ( \"Collecting payment without sample key\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:7\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"sample_key\" : \"value\" }, { \"level\" : \"INFO\" , \"location\" : \"collect.handler:10\" , \"message\" : \"Collecting payment without sample key\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" }","title":"Removing additional keys"},{"location":"core/logger/#clearing-all-state","text":"Logger is commonly initialized in the global scope. Due to Lambda Execution Context reuse , this means that custom keys can be persisted across invocations. If you want all custom keys to be deleted, you can use clear_state=True param in inject_lambda_context decorator. Info This is useful when you add multiple custom keys conditionally, instead of setting a default None value if not present. Any key with None value is automatically removed by Logger. This can have unintended side effects if you use Layers Lambda Layers code is imported before the Lambda handler. This means that clear_state=True will instruct Logger to remove any keys previously added before Lambda handler execution proceeds. You can either avoid running any code as part of Lambda Layers global scope, or override keys with their latest value as part of handler's execution. collect.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) @logger . inject_lambda_context ( clear_state = True ) def handler ( event , context ): if event . get ( \"special_key\" ): # Should only be available in the first request log # as the second request doesn't contain `special_key` logger . append_keys ( debugging_key = \"value\" ) logger . info ( \"Collecting payment\" ) #1 request 1 2 3 4 5 6 7 8 9 10 11 12 13 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:10\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"special_key\" : \"debug_key\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" } #2 request 1 2 3 4 5 6 7 8 9 10 11 12 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:10\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : false , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" }","title":"Clearing all state"},{"location":"core/logger/#logging-exceptions","text":"Use logger.exception method to log contextual information about exceptions. Logger will include exception_name and exception keys to aid troubleshooting and error enumeration. Tip You can use your preferred Log Analytics tool to enumerate and visualize exceptions across all your services using exception_name key. collect.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) try : raise ValueError ( \"something went wrong\" ) except Exception : logger . exception ( \"Received an exception\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 { \"level\" : \"ERROR\" , \"location\" : \"collect.handler:5\" , \"message\" : \"Received an exception\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"exception_name\" : \"ValueError\" , \"exception\" : \"Traceback (most recent call last):\\n File \\\"<input>\\\", line 2, in <module>\\nValueError: something went wrong\" }","title":"Logging exceptions"},{"location":"core/logger/#advanced","text":"","title":"Advanced"},{"location":"core/logger/#reusing-logger-across-your-code","text":"Logger supports inheritance via child parameter. This allows you to create multiple Loggers across your code base, and propagate changes such as new keys to all Loggers. collect.py 1 2 3 4 5 6 7 8 import shared # Creates a child logger named \"payment.shared\" from aws_lambda_powertools import Logger logger = Logger () # POWERTOOLS_SERVICE_NAME: \"payment\" def handler ( event , context ): shared . inject_payment_id ( event ) ... shared.py 1 2 3 4 5 6 from aws_lambda_powertools import Logger logger = Logger ( child = True ) # POWERTOOLS_SERVICE_NAME: \"payment\" def inject_payment_id ( event ): logger . structure_logs ( append = True , payment_id = event . get ( \"payment_id\" )) In this example, Logger will create a parent logger named payment and a child logger named payment.shared . Changes in either parent or child logger will be propagated bi-directionally. Child loggers will be named after the following convention {service}.{filename} If you forget to use child param but the service name is the same of the parent, we will return the existing parent Logger instead.","title":"Reusing Logger across your code"},{"location":"core/logger/#sampling-debug-logs","text":"Use sampling when you want to dynamically change your log level to DEBUG based on a percentage of your concurrent/cold start invocations . You can use values ranging from 0.0 to 1 (100%) when setting POWERTOOLS_LOGGER_SAMPLE_RATE env var or sample_rate parameter in Logger. When is this useful? Let's imagine a sudden spike increase in concurrency triggered a transient issue downstream. When looking into the logs you might not have enough information, and while you can adjust log levels it might not happen again. This feature takes into account transient issues where additional debugging information can be useful. Sampling decision happens at the Logger initialization. This means sampling may happen significantly more or less than depending on your traffic patterns, for example a steady low number of invocations and thus few cold starts. Note If you want Logger to calculate sampling upon every invocation, please open a feature request . collect.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Logger # Sample 10% of debug logs e.g. 0.1 logger = Logger ( service = \"payment\" , sample_rate = 0.1 ) def handler ( event , context ): logger . debug ( \"Verifying whether order_id is present\" ) logger . info ( \"Collecting payment\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \"level\" : \"DEBUG\" , \"location\" : \"collect.handler:7\" , \"message\" : \"Verifying whether order_id is present\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"sampling_rate\" : 0.1 }, { \"level\" : \"INFO\" , \"location\" : \"collect.handler:7\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494+0200\" , \"service\" : \"payment\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"sampling_rate\" : 0.1 }","title":"Sampling debug logs"},{"location":"core/logger/#lambdapowertoolsformatter","text":"Logger propagates a few formatting configurations to the built-in LambdaPowertoolsFormatter logging formatter. If you prefer configuring it separately, or you'd want to bring this JSON Formatter to another application, these are the supported settings: Parameter Description Default json_serializer function to serialize obj to a JSON formatted str json.dumps json_deserializer function to deserialize str , bytes , bytearray containing a JSON document to a Python obj json.loads json_default function to coerce unserializable values, when no custom serializer/deserializer is set str datefmt string directives (strftime) to format log timestamp %Y-%m-%d %H:%M:%S,%F%z , where %F is a custom ms directive utc set logging timestamp to UTC False log_record_order set order of log keys when logging [\"level\", \"location\", \"message\", \"timestamp\"] kwargs key-value to be included in log messages None LambdaPowertoolsFormatter.py 1 2 3 4 5 from aws_lambda_powertools import Logger from aws_lambda_powertools.logging.formatter import LambdaPowertoolsFormatter formatter = LambdaPowertoolsFormatter ( utc = True , log_record_order = [ \"message\" ]) logger = Logger ( service = \"example\" , logger_formatter = formatter )","title":"LambdaPowertoolsFormatter"},{"location":"core/logger/#migrating-from-other-loggers","text":"If you're migrating from other Loggers, there are few key points to be aware of: Service parameter , Inheriting Loggers , Overriding Log records , and Logging exceptions .","title":"Migrating from other Loggers"},{"location":"core/logger/#the-service-parameter","text":"Service is what defines the Logger name, including what the Lambda function is responsible for, or part of (e.g payment service). For Logger, the service is the logging key customers can use to search log operations for one or more functions - For example, search for all errors, or messages like X, where service is payment .","title":"The service parameter"},{"location":"core/logger/#inheriting-loggers","text":"Python Logging hierarchy happens via the dot notation: service , service.child , service.child_2 For inheritance, Logger uses a child=True parameter along with service being the same value across Loggers. For child Loggers, we introspect the name of your module where Logger(child=True, service=\"name\") is called, and we name your Logger as {service}.{filename} . Danger A common issue when migrating from other Loggers is that service might be defined in the parent Logger (no child param), and not defined in the child Logger: incorrect_logger_inheritance.py 1 2 3 4 5 6 7 8 9 10 import my_module from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) ... # my_module.py from aws_lambda_powertools import Logger logger = Logger ( child = True ) correct_logger_inheritance.py 1 2 3 4 5 6 7 8 9 10 import my_module from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) ... # my_module.py from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" , child = True ) In this case, Logger will register a Logger named payment , and a Logger named service_undefined . The latter isn't inheriting from the parent, and will have no handler, resulting in no message being logged to standard output. Tip This can be fixed by either ensuring both has the service value as payment , or simply use the environment variable POWERTOOLS_SERVICE_NAME to ensure service value will be the same across all Loggers when not explicitly set.","title":"Inheriting Loggers"},{"location":"core/logger/#overriding-log-records","text":"You might want to continue to use the same date formatting style, or override location to display the package.function_name:line_number as you previously had. Logger allows you to either change the format or suppress the following keys altogether at the initialization: location , timestamp , level , xray_trace_id . lambda_handler.py We honour standard logging library string formats . 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools import Logger date_format = \"%m/ %d /%Y %I:%M:%S %p\" location_format = \"[ %(funcName)s ] %(module)s \" # override location and timestamp format logger = Logger ( service = \"payment\" , location = location_format , datefmt = date_format ) # suppress the location key with a None value logger_two = Logger ( service = \"payment\" , location = None ) logger . info ( \"Collecting payment\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 { \"level\" : \"INFO\" , \"location\" : \"[<module>] lambda_handler\" , \"message\" : \"Collecting payment\" , \"timestamp\" : \"02/09/2021 09:25:17 AM\" , \"service\" : \"payment\" }","title":"Overriding Log records"},{"location":"core/logger/#reordering-log-keys-position","text":"You can change the order of standard Logger keys or any keys that will be appended later at runtime via the log_record_order parameter. lambda_handler.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools import Logger # make message as the first key logger = Logger ( service = \"payment\" , log_record_order = [ \"message\" ]) # make request_id that will be added later as the first key # Logger(service=\"payment\", log_record_order=[\"request_id\"]) # Default key sorting order when omit # Logger(service=\"payment\", log_record_order=[\"level\",\"location\",\"message\",\"timestamp\"]) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 { \"message\" : \"hello world\" , \"level\" : \"INFO\" , \"location\" : \"[<module>]:6\" , \"timestamp\" : \"2021-02-09 09:36:12,280\" , \"service\" : \"service_undefined\" , \"sampling_rate\" : 0.0 }","title":"Reordering log keys position"},{"location":"core/logger/#setting-timestamp-to-utc","text":"By default, this Logger and standard logging library emits records using local time timestamp. You can override this behaviour via utc parameter: app.py 1 2 3 4 5 6 7 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) logger . info ( \"Local time\" ) logger_in_utc = Logger ( service = \"payment\" , utc = True ) logger_in_utc . info ( \"GMT time zone\" )","title":"Setting timestamp to UTC"},{"location":"core/logger/#custom-function-for-unserializable-values","text":"By default, Logger uses str to handle values non-serializable by JSON. You can override this behaviour via json_default parameter by passing a Callable: collect.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools import Logger def custom_json_default ( value ): return f \"<non-serializable: { type ( value ) . __name__ } >\" class Unserializable : pass logger = Logger ( service = \"payment\" , json_default = custom_json_default ) def handler ( event , context ): logger . info ( Unserializable ()) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 { \"level\" : \"INFO\" , \"location\" : \"collect.handler:8\" , \"message\" : \"\" < n o n - serializable : U nser ializable> \"\" , \"timestamp\" : \"2021-05-03 15:17:23,632+0200\" , \"service\" : \"payment\" }","title":"Custom function for unserializable values"},{"location":"core/logger/#bring-your-own-handler","text":"By default, Logger uses StreamHandler and logs to standard output. You can override this behaviour via logger_handler parameter: collect.py 1 2 3 4 5 6 7 8 9 10 import logging from pathlib import Path from aws_lambda_powertools import Logger log_file = Path ( \"/tmp/log.json\" ) log_file_handler = logging . FileHandler ( filename = log_file ) logger = Logger ( service = \"payment\" , logger_handler = log_file_handler ) logger . info ( \"Collecting payment\" )","title":"Bring your own handler"},{"location":"core/logger/#bring-your-own-formatter","text":"By default, Logger uses LambdaPowertoolsFormatter that persists its custom structure between non-cold start invocations. There could be scenarios where the existing feature set isn't sufficient to your formatting needs. For minor changes like remapping keys after all log record processing has completed, you can override serialize method from LambdaPowertoolsFormatter : custom_formatter.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools import Logger from aws_lambda_powertools.logging.formatter import LambdaPowertoolsFormatter from typing import Dict class CustomFormatter ( LambdaPowertoolsFormatter ): def serialize ( self , log : Dict ) -> str : \"\"\"Serialize final structured log dict to JSON str\"\"\" log [ \"event\" ] = log . pop ( \"message\" ) # rename message key to event return self . json_serializer ( log ) # use configured json serializer my_formatter = CustomFormatter () logger = Logger ( service = \"example\" , logger_formatter = my_formatter ) logger . info ( \"hello\" ) For replacing the formatter entirely , you can subclass BasePowertoolsFormatter , implement append_keys method, and override format standard logging method. This ensures the current feature set of Logger like injecting Lambda context and sampling will continue to work. Info You might need to implement remove_keys method if you make use of the feature too. collect.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from aws_lambda_powertools import Logger from aws_lambda_powertools.logging.formatter import BasePowertoolsFormatter class CustomFormatter ( BasePowertoolsFormatter ): custom_format = {} # arbitrary dict to hold our structured keys def append_keys ( self , ** additional_keys ): # also used by `inject_lambda_context` decorator self . custom_format . update ( additional_keys ) # Optional unless you make use of this Logger feature def remove_keys ( self , keys : Iterable [ str ]): for key in keys : self . custom_format . pop ( key , None ) def format ( self , record : logging . LogRecord ) -> str : # noqa: A003 \"\"\"Format logging record as structured JSON str\"\"\" return json . dumps ( { \"event\" : super () . format ( record ), \"timestamp\" : self . formatTime ( record ), \"my_default_key\" : \"test\" , ** self . custom_format , } ) logger = Logger ( service = \"payment\" , logger_formatter = CustomFormatter ()) @logger . inject_lambda_context def handler ( event , context ): logger . info ( \"Collecting payment\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 { \"event\" : \"Collecting payment\" , \"timestamp\" : \"2021-05-03 11:47:12,494\" , \"my_default_key\" : \"test\" , \"cold_start\" : true , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" }","title":"Bring your own formatter"},{"location":"core/logger/#bring-your-own-json-serializer","text":"By default, Logger uses json.dumps and json.loads as serializer and deserializer respectively. There could be scenarios where you are making use of alternative JSON libraries like orjson . As parameters don't always translate well between them, you can pass any callable that receives a Dict and return a str : collect.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import orjson from aws_lambda_powertools import Logger custom_serializer = orjson . dumps custom_deserializer = orjson . loads logger = Logger ( service = \"payment\" , json_serializer = custom_serializer , json_deserializer = custom_deserializer ) # when using parameters, you can pass a partial # custom_serializer=functools.partial(orjson.dumps, option=orjson.OPT_SERIALIZE_NUMPY)","title":"Bring your own JSON serializer"},{"location":"core/logger/#built-in-correlation-id-expressions","text":"You can use any of the following built-in JMESPath expressions as part of inject_lambda_context decorator . Escaping necessary for the - character Any object key named with - must be escaped, for example request.headers.\"x-amzn-trace-id\" . Name Expression Description API_GATEWAY_REST \"requestContext.requestId\" API Gateway REST API request ID API_GATEWAY_HTTP \"requestContext.requestId\" API Gateway HTTP API request ID APPSYNC_RESOLVER 'request.headers.\"x-amzn-trace-id\"' AppSync X-Ray Trace ID APPLICATION_LOAD_BALANCER 'headers.\"x-amzn-trace-id\"' ALB X-Ray Trace ID EVENT_BRIDGE \"id\" EventBridge Event ID","title":"Built-in Correlation ID expressions"},{"location":"core/logger/#testing-your-code","text":"","title":"Testing your code"},{"location":"core/logger/#inject-lambda-context","text":"When unit testing your code that makes use of inject_lambda_context decorator, you need to pass a dummy Lambda Context, or else Logger will fail. This is a Pytest sample that provides the minimum information necessary for Logger to succeed: fake_lambda_context_for_logger.py Note that dataclasses are available in Python 3.7+ only. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from dataclasses import dataclass import pytest @pytest . fixture def lambda_context (): @dataclass class LambdaContext : function_name : str = \"test\" memory_limit_in_mb : int = 128 invoked_function_arn : str = \"arn:aws:lambda:eu-west-1:809313241:function:test\" aws_request_id : str = \"52fdfc07-2182-154f-163f-5f0f9a621d72\" return LambdaContext () def test_lambda_handler ( lambda_context ): test_event = { 'test' : 'event' } your_lambda_handler ( test_event , lambda_context ) # this will now have a Context object populated fake_lambda_context_for_logger_py36.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from collections import namedtuple import pytest @pytest . fixture def lambda_context (): lambda_context = { \"function_name\" : \"test\" , \"memory_limit_in_mb\" : 128 , \"invoked_function_arn\" : \"arn:aws:lambda:eu-west-1:809313241:function:test\" , \"aws_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , } return namedtuple ( \"LambdaContext\" , lambda_context . keys ())( * lambda_context . values ()) def test_lambda_handler ( lambda_context ): test_event = { 'test' : 'event' } # this will now have a Context object populated your_lambda_handler ( test_event , lambda_context ) Tip If you're using pytest and are looking to assert plain log messages, do check out the built-in caplog fixture .","title":"Inject Lambda Context"},{"location":"core/logger/#pytest-live-log-feature","text":"Pytest Live Log feature duplicates emitted log messages in order to style log statements according to their levels, for this to work use POWERTOOLS_LOG_DEDUPLICATION_DISABLED env var. shell 1 POWERTOOLS_LOG_DEDUPLICATION_DISABLED = \"1\" pytest -o log_cli = 1 Warning This feature should be used with care, as it explicitly disables our ability to filter propagated messages to the root logger (if configured).","title":"Pytest live log feature"},{"location":"core/logger/#faq","text":"How can I enable boto3 and botocore library logging? You can enable the botocore and boto3 logs by using the set_stream_logger method, this method will add a stream handler for the given name and level to the logging module. By default, this logs all boto3 messages to stdout. log_botocore_and_boto3.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from typing import Dict , List from aws_lambda_powertools.utilities.typing import LambdaContext from aws_lambda_powertools import Logger import boto3 boto3 . set_stream_logger () boto3 . set_stream_logger ( 'botocore' ) logger = Logger () client = boto3 . client ( 's3' ) def handler ( event : Dict , context : LambdaContext ) -> List : response = client . list_buckets () return response . get ( \"Buckets\" , []) What's the difference between append_keys and extra ? Keys added with append_keys will persist across multiple log messages while keys added via extra will only be available in a given log message operation. Here's an example where we persist payment_id not request_id . Note that payment_id remains in both log messages while booking_id is only available in the first message. lambda_handler.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) def handler ( event , context ): logger . append_keys ( payment_id = \"123456789\" ) try : booking_id = book_flight () logger . info ( \"Flight booked successfully\" , extra = { \"booking_id\" : booking_id }) except BookingReservationError : ... logger . info ( \"goodbye\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"level\" : \"INFO\" , \"location\" : \"<module>:10\" , \"message\" : \"Flight booked successfully\" , \"timestamp\" : \"2021-01-12 14:09:10,859\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"payment_id\" : \"123456789\" , \"booking_id\" : \"75edbad0-0857-4fc9-b547-6180e2f7959b\" }, { \"level\" : \"INFO\" , \"location\" : \"<module>:14\" , \"message\" : \"goodbye\" , \"timestamp\" : \"2021-01-12 14:09:10,860\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"payment_id\" : \"123456789\" } How do I aggregate and search Powertools logs across accounts? As of now, ElasticSearch (ELK) or 3rd party solutions are best suited to this task. Please see this discussion for more information: https://github.com/awslabs/aws-lambda-powertools-python/issues/460","title":"FAQ"},{"location":"core/metrics/","text":"Metrics creates custom metrics asynchronously by logging metrics to standard output following Amazon CloudWatch Embedded Metric Format (EMF) . These metrics can be visualized through Amazon CloudWatch Console . Key features \u00b6 Aggregate up to 100 metrics using a single CloudWatch EMF object (large JSON blob) Validate against common metric definitions mistakes (metric unit, values, max dimensions, max metrics, etc) Metrics are created asynchronously by CloudWatch service, no custom stacks needed Context manager to create a one off metric with a different dimension Terminologies \u00b6 If you're new to Amazon CloudWatch, there are two terminologies you must be aware of before using this utility: Namespace . It's the highest level container that will group multiple metrics from multiple services for a given application, for example ServerlessEcommerce . Dimensions . Metrics metadata in key-value format. They help you slice and dice metrics visualization, for example ColdStart metric by Payment service . Metric terminology, visually explained Getting started \u00b6 Metric has two global settings that will be used across all metrics emitted: Setting Description Environment variable Constructor parameter Metric namespace Logical container where all metrics will be placed e.g. ServerlessAirline POWERTOOLS_METRICS_NAMESPACE namespace Service Optionally, sets service metric dimension across all metrics e.g. payment POWERTOOLS_SERVICE_NAME service Use your application or main service as the metric namespace to easily group all metrics Example using AWS Serverless Application Model (SAM) template.yml 1 2 3 4 5 6 7 8 9 Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : Runtime : nodejs14.x Environment : Variables : POWERTOOLS_SERVICE_NAME : payment POWERTOOLS_METRICS_NAMESPACE : ServerlessAirline app.py 1 2 3 4 5 6 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics () # Sets metric namespace and service via env var # OR metrics = Metrics ( namespace = \"ServerlessAirline\" , service = \"orders\" ) # Sets metric namespace, and service as a metric dimension You can initialize Metrics anywhere in your code - It'll keep track of your aggregate metrics in memory. Creating metrics \u00b6 You can create metrics using add_metric , and you can create dimensions for all your aggregate metrics using add_dimension method. Metrics 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) Metrics with custom dimensions 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_dimension ( name = \"environment\" , value = \"prod\" ) metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) Autocomplete Metric Units MetricUnit enum facilitate finding a supported metric unit by CloudWatch. Alternatively, you can pass the value as a string if you already know them e.g. \"Count\". Metrics overflow CloudWatch EMF supports a max of 100 metrics per batch. Metrics utility will flush all metrics when adding the 100th metric. Subsequent metrics, e.g. 101th, will be aggregated into a new EMF object, for your convenience. Do not create metrics or dimensions outside the handler Metrics or dimensions added in the global scope will only be added during cold start. Disregard if you that's the intended behaviour. Adding default dimensions \u00b6 You can use either set_default_dimensions method or default_permissions parameter in log_metrics decorator to persist dimensions across Lambda invocations. If you'd like to remove them at some point, you can use clear_default_dimensions method. set_default_dimensions method 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) metrics . set_default_dimensions ( environment = \"prod\" , another = \"one\" ) @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) with log_metrics decorator 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) DEFAULT_DIMENSIONS = { \"environment\" : \"prod\" , \"another\" : \"one\" } @metrics . log_metrics ( default_dimensions = DEFAULT_DIMENSIONS ) def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) Flushing metrics \u00b6 As you finish adding all your metrics, you need to serialize and flush them to standard output. You can do that automatically with the log_metrics decorator. This decorator also validates , serializes , and flushes all your metrics. During metrics validation, if no metrics are provided then a warning will be logged, but no exception will be raised. app.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"ExampleService\" ) @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"BookingConfirmation\" , unit = MetricUnit . Count , value = 1 ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 { \"BookingConfirmation\" : 1.0 , \"_aws\" : { \"Timestamp\" : 1592234975665 , \"CloudWatchMetrics\" : [ { \"Namespace\" : \"ExampleApplication\" , \"Dimensions\" : [ [ \"service\" ] ], \"Metrics\" : [ { \"Name\" : \"BookingConfirmation\" , \"Unit\" : \"Count\" } ] } ] }, \"service\" : \"ExampleService\" } Metric validation If metrics are provided, and any of the following criteria are not met, SchemaValidationError exception will be raised: Maximum of 9 dimensions Namespace is set, and no more than one Metric units must be supported by CloudWatch Raising SchemaValidationError on empty metrics \u00b6 If you want to ensure that at least one metric is emitted, you can pass raise_on_empty_metrics to the log_metrics decorator: app.py 1 2 3 4 5 6 7 from aws_lambda_powertools.metrics import Metrics metrics = Metrics () @metrics . log_metrics ( raise_on_empty_metrics = True ) def lambda_handler ( evt , ctx ): ... Suppressing warning messages on empty metrics If you expect your function to execute without publishing metrics every time, you can suppress the warning with warnings.filterwarnings(\"ignore\", \"No metrics to publish*\") . Nesting multiple middlewares \u00b6 When using multiple middlewares, use log_metrics as your last decorator wrapping all subsequent ones to prevent early Metric validations when code hasn't been run yet. nested_middlewares.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools import Metrics , Tracer from aws_lambda_powertools.metrics import MetricUnit tracer = Tracer ( service = \"booking\" ) metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) @metrics . log_metrics @tracer . capture_lambda_handler def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"BookingConfirmation\" , unit = MetricUnit . Count , value = 1 ) Capturing cold start metric \u00b6 You can optionally capture cold start metrics with log_metrics decorator via capture_cold_start_metric param. app.py 1 2 3 4 5 6 7 from aws_lambda_powertools import Metrics metrics = Metrics ( service = \"ExampleService\" ) @metrics . log_metrics ( capture_cold_start_metric = True ) def lambda_handler ( evt , ctx ): ... If it's a cold start invocation, this feature will: Create a separate EMF blob solely containing a metric named ColdStart Add function_name and service dimensions This has the advantage of keeping cold start metric separate from your application metrics, where you might have unrelated dimensions. We do not emit 0 as a value for ColdStart metric for cost reasons. Let us know if you'd prefer a flag to override it Advanced \u00b6 Adding metadata \u00b6 You can add high-cardinality data as part of your Metrics log with add_metadata method. This is useful when you want to search highly contextual information along with your metrics in your logs. Info This will not be available during metrics visualization - Use dimensions for this purpose app.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) metrics . add_metadata ( key = \"booking_id\" , value = \"booking_uuid\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { \"SuccessfulBooking\" : 1.0 , \"_aws\" : { \"Timestamp\" : 1592234975665 , \"CloudWatchMetrics\" : [ { \"Namespace\" : \"ExampleApplication\" , \"Dimensions\" : [ [ \"service\" ] ], \"Metrics\" : [ { \"Name\" : \"SuccessfulBooking\" , \"Unit\" : \"Count\" } ] } ] }, \"service\" : \"booking\" , \"booking_id\" : \"booking_uuid\" } Single metric with a different dimension \u00b6 CloudWatch EMF uses the same dimensions across all your metrics. Use single_metric if you have a metric that should have different dimensions. Info Generally, this would be an edge case since you pay for unique metric . Keep the following formula in mind: unique metric = (metric_name + dimension_name + dimension_value) single_metric.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools import single_metric from aws_lambda_powertools.metrics import MetricUnit def lambda_handler ( evt , ctx ): with single_metric ( name = \"ColdStart\" , unit = MetricUnit . Count , value = 1 , namespace = \"ExampleApplication\" ) as metric : metric . add_dimension ( name = \"function_context\" , value = \"$LATEST\" ) ... Flushing metrics manually \u00b6 If you prefer not to use log_metrics because you might want to encapsulate additional logic when doing so, you can manually flush and clear metrics as follows: Warning Metrics, dimensions and namespace validation still applies. manual_metric_serialization.py 1 2 3 4 5 6 7 8 9 10 11 import json from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"ColdStart\" , unit = MetricUnit . Count , value = 1 ) your_metrics_object = metrics . serialize_metric_set () metrics . clear_metrics () print ( json . dumps ( your_metrics_object )) Testing your code \u00b6 Environment variables \u00b6 Use POWERTOOLS_METRICS_NAMESPACE and POWERTOOLS_SERVICE_NAME env vars when unit testing your code to ensure metric namespace and dimension objects are created, and your code doesn't fail validation. shell 1 POWERTOOLS_SERVICE_NAME = \"Example\" POWERTOOLS_METRICS_NAMESPACE = \"Application\" python -m pytest If you prefer setting environment variable for specific tests, and are using Pytest, you can use monkeypatch fixture: pytest_env_var.py 1 2 3 4 5 6 def test_namespace_env_var ( monkeypatch ): # Set POWERTOOLS_METRICS_NAMESPACE before initializating Metrics monkeypatch . setenv ( \"POWERTOOLS_METRICS_NAMESPACE\" , namespace ) metrics = Metrics () ... Ignore this, if you are explicitly setting namespace/default dimension via namespace and service parameters: metrics = Metrics(namespace=ApplicationName, service=ServiceName) Clearing metrics \u00b6 Metrics keep metrics in memory across multiple instances. If you need to test this behaviour, you can use the following Pytest fixture to ensure metrics are reset incl. cold start: pytest_metrics_reset_fixture.py 1 2 3 4 5 6 7 8 @pytest . fixture ( scope = \"function\" , autouse = True ) def reset_metric_set (): # Clear out every metric data prior to every test metrics = Metrics () metrics . clear_metrics () metrics_global . is_cold_start = True # ensure each test has cold start metrics . clear_default_dimensions () # remove persisted default dimensions, if any yield Functional testing \u00b6 As metrics are logged to standard output, you can read standard output and assert whether metrics are present. Here's an example using pytest with capsys built-in fixture: Assert single EMF blob with pytest.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit import json def test_log_metrics ( capsys ): # GIVEN Metrics is initialized metrics = Metrics ( namespace = \"ServerlessAirline\" ) # WHEN we utilize log_metrics to serialize # and flush all metrics at the end of a function execution @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) metrics . add_dimension ( name = \"environment\" , value = \"prod\" ) lambda_handler ({}, {}) log = capsys . readouterr () . out . strip () # remove any extra line metrics_output = json . loads ( log ) # deserialize JSON str # THEN we should have no exceptions # and a valid EMF object should be flushed correctly assert \"SuccessfulBooking\" in log # basic string assertion in JSON str assert \"SuccessfulBooking\" in metrics_output [ \"_aws\" ][ \"CloudWatchMetrics\" ][ 0 ][ \"Metrics\" ][ 0 ][ \"Name\" ] Assert multiple EMF blobs with pytest 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit from collections import namedtuple import json def capture_metrics_output_multiple_emf_objects ( capsys ): return [ json . loads ( line . strip ()) for line in capsys . readouterr () . out . split ( \" \\n \" ) if line ] def test_log_metrics ( capsys ): # GIVEN Metrics is initialized metrics = Metrics ( namespace = \"ServerlessAirline\" ) # WHEN log_metrics is used with capture_cold_start_metric @metrics . log_metrics ( capture_cold_start_metric = True ) def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) metrics . add_dimension ( name = \"environment\" , value = \"prod\" ) # log_metrics uses function_name property from context to add as a dimension for cold start metric LambdaContext = namedtuple ( \"LambdaContext\" , \"function_name\" ) lambda_handler ({}, LambdaContext ( \"example_fn\" ) cold_start_blob , custom_metrics_blob = capture_metrics_output_multiple_emf_objects ( capsys ) # THEN ColdStart metric and function_name dimension should be logged # in a separate EMF blob than the application metrics assert cold_start_blob [ \"ColdStart\" ] == [ 1.0 ] assert cold_start_blob [ \"function_name\" ] == \"example_fn\" assert \"SuccessfulBooking\" in custom_metrics_blob # as per previous example For more elaborate assertions and comparisons, check out our functional testing for Metrics utility","title":"Metrics"},{"location":"core/metrics/#key-features","text":"Aggregate up to 100 metrics using a single CloudWatch EMF object (large JSON blob) Validate against common metric definitions mistakes (metric unit, values, max dimensions, max metrics, etc) Metrics are created asynchronously by CloudWatch service, no custom stacks needed Context manager to create a one off metric with a different dimension","title":"Key features"},{"location":"core/metrics/#terminologies","text":"If you're new to Amazon CloudWatch, there are two terminologies you must be aware of before using this utility: Namespace . It's the highest level container that will group multiple metrics from multiple services for a given application, for example ServerlessEcommerce . Dimensions . Metrics metadata in key-value format. They help you slice and dice metrics visualization, for example ColdStart metric by Payment service . Metric terminology, visually explained","title":"Terminologies"},{"location":"core/metrics/#getting-started","text":"Metric has two global settings that will be used across all metrics emitted: Setting Description Environment variable Constructor parameter Metric namespace Logical container where all metrics will be placed e.g. ServerlessAirline POWERTOOLS_METRICS_NAMESPACE namespace Service Optionally, sets service metric dimension across all metrics e.g. payment POWERTOOLS_SERVICE_NAME service Use your application or main service as the metric namespace to easily group all metrics Example using AWS Serverless Application Model (SAM) template.yml 1 2 3 4 5 6 7 8 9 Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : Runtime : nodejs14.x Environment : Variables : POWERTOOLS_SERVICE_NAME : payment POWERTOOLS_METRICS_NAMESPACE : ServerlessAirline app.py 1 2 3 4 5 6 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics () # Sets metric namespace and service via env var # OR metrics = Metrics ( namespace = \"ServerlessAirline\" , service = \"orders\" ) # Sets metric namespace, and service as a metric dimension You can initialize Metrics anywhere in your code - It'll keep track of your aggregate metrics in memory.","title":"Getting started"},{"location":"core/metrics/#creating-metrics","text":"You can create metrics using add_metric , and you can create dimensions for all your aggregate metrics using add_dimension method. Metrics 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) Metrics with custom dimensions 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_dimension ( name = \"environment\" , value = \"prod\" ) metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) Autocomplete Metric Units MetricUnit enum facilitate finding a supported metric unit by CloudWatch. Alternatively, you can pass the value as a string if you already know them e.g. \"Count\". Metrics overflow CloudWatch EMF supports a max of 100 metrics per batch. Metrics utility will flush all metrics when adding the 100th metric. Subsequent metrics, e.g. 101th, will be aggregated into a new EMF object, for your convenience. Do not create metrics or dimensions outside the handler Metrics or dimensions added in the global scope will only be added during cold start. Disregard if you that's the intended behaviour.","title":"Creating metrics"},{"location":"core/metrics/#adding-default-dimensions","text":"You can use either set_default_dimensions method or default_permissions parameter in log_metrics decorator to persist dimensions across Lambda invocations. If you'd like to remove them at some point, you can use clear_default_dimensions method. set_default_dimensions method 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) metrics . set_default_dimensions ( environment = \"prod\" , another = \"one\" ) @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) with log_metrics decorator 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) DEFAULT_DIMENSIONS = { \"environment\" : \"prod\" , \"another\" : \"one\" } @metrics . log_metrics ( default_dimensions = DEFAULT_DIMENSIONS ) def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 )","title":"Adding default dimensions"},{"location":"core/metrics/#flushing-metrics","text":"As you finish adding all your metrics, you need to serialize and flush them to standard output. You can do that automatically with the log_metrics decorator. This decorator also validates , serializes , and flushes all your metrics. During metrics validation, if no metrics are provided then a warning will be logged, but no exception will be raised. app.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"ExampleService\" ) @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"BookingConfirmation\" , unit = MetricUnit . Count , value = 1 ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 { \"BookingConfirmation\" : 1.0 , \"_aws\" : { \"Timestamp\" : 1592234975665 , \"CloudWatchMetrics\" : [ { \"Namespace\" : \"ExampleApplication\" , \"Dimensions\" : [ [ \"service\" ] ], \"Metrics\" : [ { \"Name\" : \"BookingConfirmation\" , \"Unit\" : \"Count\" } ] } ] }, \"service\" : \"ExampleService\" } Metric validation If metrics are provided, and any of the following criteria are not met, SchemaValidationError exception will be raised: Maximum of 9 dimensions Namespace is set, and no more than one Metric units must be supported by CloudWatch","title":"Flushing metrics"},{"location":"core/metrics/#raising-schemavalidationerror-on-empty-metrics","text":"If you want to ensure that at least one metric is emitted, you can pass raise_on_empty_metrics to the log_metrics decorator: app.py 1 2 3 4 5 6 7 from aws_lambda_powertools.metrics import Metrics metrics = Metrics () @metrics . log_metrics ( raise_on_empty_metrics = True ) def lambda_handler ( evt , ctx ): ... Suppressing warning messages on empty metrics If you expect your function to execute without publishing metrics every time, you can suppress the warning with warnings.filterwarnings(\"ignore\", \"No metrics to publish*\") .","title":"Raising SchemaValidationError on empty metrics"},{"location":"core/metrics/#nesting-multiple-middlewares","text":"When using multiple middlewares, use log_metrics as your last decorator wrapping all subsequent ones to prevent early Metric validations when code hasn't been run yet. nested_middlewares.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools import Metrics , Tracer from aws_lambda_powertools.metrics import MetricUnit tracer = Tracer ( service = \"booking\" ) metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) @metrics . log_metrics @tracer . capture_lambda_handler def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"BookingConfirmation\" , unit = MetricUnit . Count , value = 1 )","title":"Nesting multiple middlewares"},{"location":"core/metrics/#capturing-cold-start-metric","text":"You can optionally capture cold start metrics with log_metrics decorator via capture_cold_start_metric param. app.py 1 2 3 4 5 6 7 from aws_lambda_powertools import Metrics metrics = Metrics ( service = \"ExampleService\" ) @metrics . log_metrics ( capture_cold_start_metric = True ) def lambda_handler ( evt , ctx ): ... If it's a cold start invocation, this feature will: Create a separate EMF blob solely containing a metric named ColdStart Add function_name and service dimensions This has the advantage of keeping cold start metric separate from your application metrics, where you might have unrelated dimensions. We do not emit 0 as a value for ColdStart metric for cost reasons. Let us know if you'd prefer a flag to override it","title":"Capturing cold start metric"},{"location":"core/metrics/#advanced","text":"","title":"Advanced"},{"location":"core/metrics/#adding-metadata","text":"You can add high-cardinality data as part of your Metrics log with add_metadata method. This is useful when you want to search highly contextual information along with your metrics in your logs. Info This will not be available during metrics visualization - Use dimensions for this purpose app.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) metrics . add_metadata ( key = \"booking_id\" , value = \"booking_uuid\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { \"SuccessfulBooking\" : 1.0 , \"_aws\" : { \"Timestamp\" : 1592234975665 , \"CloudWatchMetrics\" : [ { \"Namespace\" : \"ExampleApplication\" , \"Dimensions\" : [ [ \"service\" ] ], \"Metrics\" : [ { \"Name\" : \"SuccessfulBooking\" , \"Unit\" : \"Count\" } ] } ] }, \"service\" : \"booking\" , \"booking_id\" : \"booking_uuid\" }","title":"Adding metadata"},{"location":"core/metrics/#single-metric-with-a-different-dimension","text":"CloudWatch EMF uses the same dimensions across all your metrics. Use single_metric if you have a metric that should have different dimensions. Info Generally, this would be an edge case since you pay for unique metric . Keep the following formula in mind: unique metric = (metric_name + dimension_name + dimension_value) single_metric.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools import single_metric from aws_lambda_powertools.metrics import MetricUnit def lambda_handler ( evt , ctx ): with single_metric ( name = \"ColdStart\" , unit = MetricUnit . Count , value = 1 , namespace = \"ExampleApplication\" ) as metric : metric . add_dimension ( name = \"function_context\" , value = \"$LATEST\" ) ...","title":"Single metric with a different dimension"},{"location":"core/metrics/#flushing-metrics-manually","text":"If you prefer not to use log_metrics because you might want to encapsulate additional logic when doing so, you can manually flush and clear metrics as follows: Warning Metrics, dimensions and namespace validation still applies. manual_metric_serialization.py 1 2 3 4 5 6 7 8 9 10 11 import json from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"ColdStart\" , unit = MetricUnit . Count , value = 1 ) your_metrics_object = metrics . serialize_metric_set () metrics . clear_metrics () print ( json . dumps ( your_metrics_object ))","title":"Flushing metrics manually"},{"location":"core/metrics/#testing-your-code","text":"","title":"Testing your code"},{"location":"core/metrics/#environment-variables","text":"Use POWERTOOLS_METRICS_NAMESPACE and POWERTOOLS_SERVICE_NAME env vars when unit testing your code to ensure metric namespace and dimension objects are created, and your code doesn't fail validation. shell 1 POWERTOOLS_SERVICE_NAME = \"Example\" POWERTOOLS_METRICS_NAMESPACE = \"Application\" python -m pytest If you prefer setting environment variable for specific tests, and are using Pytest, you can use monkeypatch fixture: pytest_env_var.py 1 2 3 4 5 6 def test_namespace_env_var ( monkeypatch ): # Set POWERTOOLS_METRICS_NAMESPACE before initializating Metrics monkeypatch . setenv ( \"POWERTOOLS_METRICS_NAMESPACE\" , namespace ) metrics = Metrics () ... Ignore this, if you are explicitly setting namespace/default dimension via namespace and service parameters: metrics = Metrics(namespace=ApplicationName, service=ServiceName)","title":"Environment variables"},{"location":"core/metrics/#clearing-metrics","text":"Metrics keep metrics in memory across multiple instances. If you need to test this behaviour, you can use the following Pytest fixture to ensure metrics are reset incl. cold start: pytest_metrics_reset_fixture.py 1 2 3 4 5 6 7 8 @pytest . fixture ( scope = \"function\" , autouse = True ) def reset_metric_set (): # Clear out every metric data prior to every test metrics = Metrics () metrics . clear_metrics () metrics_global . is_cold_start = True # ensure each test has cold start metrics . clear_default_dimensions () # remove persisted default dimensions, if any yield","title":"Clearing metrics"},{"location":"core/metrics/#functional-testing","text":"As metrics are logged to standard output, you can read standard output and assert whether metrics are present. Here's an example using pytest with capsys built-in fixture: Assert single EMF blob with pytest.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit import json def test_log_metrics ( capsys ): # GIVEN Metrics is initialized metrics = Metrics ( namespace = \"ServerlessAirline\" ) # WHEN we utilize log_metrics to serialize # and flush all metrics at the end of a function execution @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) metrics . add_dimension ( name = \"environment\" , value = \"prod\" ) lambda_handler ({}, {}) log = capsys . readouterr () . out . strip () # remove any extra line metrics_output = json . loads ( log ) # deserialize JSON str # THEN we should have no exceptions # and a valid EMF object should be flushed correctly assert \"SuccessfulBooking\" in log # basic string assertion in JSON str assert \"SuccessfulBooking\" in metrics_output [ \"_aws\" ][ \"CloudWatchMetrics\" ][ 0 ][ \"Metrics\" ][ 0 ][ \"Name\" ] Assert multiple EMF blobs with pytest 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit from collections import namedtuple import json def capture_metrics_output_multiple_emf_objects ( capsys ): return [ json . loads ( line . strip ()) for line in capsys . readouterr () . out . split ( \" \\n \" ) if line ] def test_log_metrics ( capsys ): # GIVEN Metrics is initialized metrics = Metrics ( namespace = \"ServerlessAirline\" ) # WHEN log_metrics is used with capture_cold_start_metric @metrics . log_metrics ( capture_cold_start_metric = True ) def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) metrics . add_dimension ( name = \"environment\" , value = \"prod\" ) # log_metrics uses function_name property from context to add as a dimension for cold start metric LambdaContext = namedtuple ( \"LambdaContext\" , \"function_name\" ) lambda_handler ({}, LambdaContext ( \"example_fn\" ) cold_start_blob , custom_metrics_blob = capture_metrics_output_multiple_emf_objects ( capsys ) # THEN ColdStart metric and function_name dimension should be logged # in a separate EMF blob than the application metrics assert cold_start_blob [ \"ColdStart\" ] == [ 1.0 ] assert cold_start_blob [ \"function_name\" ] == \"example_fn\" assert \"SuccessfulBooking\" in custom_metrics_blob # as per previous example For more elaborate assertions and comparisons, check out our functional testing for Metrics utility","title":"Functional testing"},{"location":"core/tracer/","text":"Tracer is an opinionated thin wrapper for AWS X-Ray Python SDK . Key features \u00b6 Auto capture cold start as annotation, and responses or full exceptions as metadata Auto-disable when not running in AWS Lambda environment Support tracing async methods, generators, and context managers Auto patch supported modules by AWS X-Ray Getting started \u00b6 Permissions \u00b6 Before your use this utility, your AWS Lambda function must have permissions to send traces to AWS X-Ray. Example using AWS Serverless Application Model (SAM) template.yml 1 2 3 4 5 6 7 8 9 Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : Runtime : nodejs14.x Tracing : Active Environment : Variables : POWERTOOLS_SERVICE_NAME : example Lambda handler \u00b6 You can quickly start by importing the Tracer class, initialize it outside the Lambda handler, and use capture_lambda_handler decorator. index.ts 1 2 3 4 5 6 7 8 9 10 11 import { Tracer } from '@aws-lambda-powertools/tracer' ; const tracer = Tracer (); // Sets service via env var // OR tracer = Tracer({ service: 'example' }); @tracer . capture_lambda_handler const handler = ( event , context ) => { const chargeId = event . chargeId ; const payment = collectPayment ( chargeId ); ... } When using this capture_lambda_handler decorator, Tracer performs these additional tasks to ease operations: Creates a ColdStart annotation to easily filter traces that have had an initialization overhead Captures any response, or full exceptions generated by the handler, and include as tracing metadata Annotations & Metadata \u00b6 Annotations are key-values associated with traces and indexed by AWS X-Ray. You can use them to filter traces and to create Trace Groups to slice and dice your transactions. Metadata are key-values also associated with traces but not indexed by AWS X-Ray. You can use them to add additional context for an operation using any native object. Annotations You can add annotations using putAnnotation method. 1 2 3 4 5 6 7 8 9 import { Tracer } from '@aws-lambda-powertools/tracer' ; const tracer = Tracer () @tracer . capture_lambda_handler const handler = ( event , context ) => { ... tracer . addAnnotation ( 'payment_response' , 'SUCCESS' ); } Metadata You can add metadata using putMetadata method. 1 2 3 4 5 6 7 8 9 10 import { Tracer } from '@aws-lambda-powertools/tracer' ; const tracer = Tracer () @tracer . capture_lambda_handler const handler = ( event , context ) => { ... const res = someLogic (); tracer . putMetadata ( 'payment_response' , res ); } Synchronous functions \u00b6 You can trace synchronous functions using the capture_method decorator. Warning When capture_response is enabled, the function response will be read and serialized as json. The serialization is performed by the aws-xray-sdk which uses the jsonpickle module. This can cause unintended consequences if there are side effects to recursively reading the returned value, for example if the decorated function response contains a file-like object or a StreamingBody for S3 objects. 1 2 3 4 5 @tracer . capture_method def collect_payment ( charge_id ): ret = requests . post ( PAYMENT_ENDPOINT ) # logic tracer . put_annotation ( \"PAYMENT_STATUS\" , \"SUCCESS\" ) # custom annotation return ret Asynchronous and generator functions \u00b6 Warning We do not support async Lambda handler - Lambda handler itself must be synchronous You can trace asynchronous functions and generator functions (including context managers) using capture_method . Async 1 2 3 4 5 6 7 8 9 import asyncio import contextlib from aws_lambda_powertools import Tracer tracer = Tracer () @tracer . capture_method async def collect_payment (): ... Context manager 1 2 3 4 5 6 7 8 9 10 11 import asyncio import contextlib from aws_lambda_powertools import Tracer tracer = Tracer () @contextlib . contextmanager @tracer . capture_method def collect_payment_ctxman (): yield result ... Generators 1 2 3 4 5 6 7 8 9 10 import asyncio import contextlib from aws_lambda_powertools import Tracer tracer = Tracer () @tracer . capture_method def collect_payment_gen (): yield result ... The decorator will detect whether your function is asynchronous, a generator, or a context manager and adapt its behaviour accordingly. app.py 1 2 3 4 5 6 7 8 @tracer . capture_lambda_handler def handler ( evt , ctx ): asyncio . run ( collect_payment ()) with collect_payment_ctxman as result : do_something_with ( result ) another_result = list ( collect_payment_gen ()) Advanced \u00b6 Patching modules \u00b6 Tracer automatically patches all supported libraries by X-Ray during initialization, by default. Underneath, AWS X-Ray SDK checks whether a supported library has been imported before patching. If you're looking to shave a few microseconds, or milliseconds depending on your function memory configuration, you can patch specific modules using patch_modules param: app.py 1 2 3 4 5 6 7 import boto3 import requests from aws_lambda_powertools import Tracer modules_to_be_patched = [ \"boto3\" , \"requests\" ] tracer = Tracer ( patch_modules = modules_to_be_patched ) Disabling response auto-capture \u00b6 New in 1.9.0 Use capture_response=False parameter in both capture_lambda_handler and capture_method decorators to instruct Tracer not to serialize function responses as metadata. This is commonly useful in three scenarios You might return sensitive information you don't want it to be added to your traces You might manipulate streaming objects that can be read only once ; this prevents subsequent calls from being empty You might return more than 64K of data e.g., message too long error sensitive_data_scenario.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Tracer @tracer . capture_method ( capture_response = False ) def fetch_sensitive_information (): return \"sensitive_information\" @tracer . capture_lambda_handler ( capture_response = False ) def handler ( event , context ): sensitive_information = fetch_sensitive_information () streaming_object_scenario.py 1 2 3 4 5 6 7 from aws_lambda_powertools import Tracer @tracer . capture_method ( capture_response = False ) def get_s3_object ( bucket_name , object_key ): s3 = boto3 . client ( \"s3\" ) s3_object = get_object ( Bucket = bucket_name , Key = object_key ) return s3_object Disabling exception auto-capture \u00b6 New in 1.10.0 Use capture_error=False parameter in both capture_lambda_handler and capture_method decorators to instruct Tracer not to serialize exceptions as metadata. Commonly useful in one scenario You might return sensitive information from exceptions, stack traces you might not control sensitive_data_exception.py 1 2 3 4 5 from aws_lambda_powertools import Tracer @tracer . capture_lambda_handler ( capture_error = False ) def handler ( event , context ): raise ValueError ( \"some sensitive info in the stack trace...\" ) Tracing aiohttp requests \u00b6 Info This snippet assumes you have aiohttp as a dependency You can use aiohttp_trace_config function to create a valid aiohttp trace_config object . This is necessary since X-Ray utilizes aiohttp trace hooks to capture requests end-to-end. aiohttp_example.py 1 2 3 4 5 6 7 8 9 10 11 12 13 import asyncio import aiohttp from aws_lambda_powertools import Tracer from aws_lambda_powertools.tracing import aiohttp_trace_config tracer = Tracer () async def aiohttp_task (): async with aiohttp . ClientSession ( trace_configs = [ aiohttp_trace_config ()]) as session : async with session . get ( \"https://httpbin.org/json\" ) as resp : resp = await resp . json () return resp Escape hatch mechanism \u00b6 You can use tracer.provider attribute to access all methods provided by AWS X-Ray xray_recorder object. This is useful when you need a feature available in X-Ray that is not available in the Tracer utility, for example thread-safe , or context managers . escape_hatch_context_manager_example.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Tracer tracer = Tracer () @tracer . capture_lambda_handler def handler ( event , context ): with tracer . provider . in_subsegment ( '## custom subsegment' ) as subsegment : ret = some_work () subsegment . put_metadata ( 'response' , ret ) Concurrent asynchronous functions \u00b6 Warning As of now, X-Ray SDK will raise an exception when async functions are run and traced concurrently A safe workaround mechanism is to use in_subsegment_async available via Tracer escape hatch ( tracer.provider ). concurrent_async_workaround.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import asyncio from aws_lambda_powertools import Tracer tracer = Tracer () async def another_async_task (): async with tracer . provider . in_subsegment_async ( \"## another_async_task\" ) as subsegment : subsegment . put_annotation ( key = \"key\" , value = \"value\" ) subsegment . put_metadata ( key = \"key\" , value = \"value\" , namespace = \"namespace\" ) ... async def another_async_task_2 (): ... @tracer . capture_method async def collect_payment ( charge_id ): asyncio . gather ( another_async_task (), another_async_task_2 ()) ... Reusing Tracer across your code \u00b6 Tracer keeps a copy of its configuration after the first initialization. This is useful for scenarios where you want to use Tracer in more than one location across your code base. Warning When reusing Tracer in Lambda Layers, or in multiple modules, do not set auto_patch=False , because import order matters. This can result in the first Tracer config being inherited by new instances, and their modules not being patched. handler.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Tracer from payment import collect_payment tracer = Tracer ( service = \"payment\" ) @tracer . capture_lambda_handler def handler ( event , context ): charge_id = event . get ( 'charge_id' ) payment = collect_payment ( charge_id ) payment.py A new instance of Tracer will be created but will reuse the previous Tracer instance configuration, similar to a Singleton. 1 2 3 4 5 6 7 from aws_lambda_powertools import Tracer tracer = Tracer ( service = \"payment\" ) @tracer . capture_method def collect_payment ( charge_id : str ): ... Testing your code \u00b6 Tracer is disabled by default when not running in the AWS Lambda environment - This means no code changes or environment variables to be set. Tips \u00b6 Use annotations on key operations to slice and dice traces, create unique views, and create metrics from it via Trace Groups Use a namespace when adding metadata to group data more easily Annotations and metadata are added to the current subsegment opened. If you want them in a specific subsegment, use a context manager via the escape hatch mechanism","title":"Tracer"},{"location":"core/tracer/#key-features","text":"Auto capture cold start as annotation, and responses or full exceptions as metadata Auto-disable when not running in AWS Lambda environment Support tracing async methods, generators, and context managers Auto patch supported modules by AWS X-Ray","title":"Key features"},{"location":"core/tracer/#getting-started","text":"","title":"Getting started"},{"location":"core/tracer/#permissions","text":"Before your use this utility, your AWS Lambda function must have permissions to send traces to AWS X-Ray. Example using AWS Serverless Application Model (SAM) template.yml 1 2 3 4 5 6 7 8 9 Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : Runtime : nodejs14.x Tracing : Active Environment : Variables : POWERTOOLS_SERVICE_NAME : example","title":"Permissions"},{"location":"core/tracer/#lambda-handler","text":"You can quickly start by importing the Tracer class, initialize it outside the Lambda handler, and use capture_lambda_handler decorator. index.ts 1 2 3 4 5 6 7 8 9 10 11 import { Tracer } from '@aws-lambda-powertools/tracer' ; const tracer = Tracer (); // Sets service via env var // OR tracer = Tracer({ service: 'example' }); @tracer . capture_lambda_handler const handler = ( event , context ) => { const chargeId = event . chargeId ; const payment = collectPayment ( chargeId ); ... } When using this capture_lambda_handler decorator, Tracer performs these additional tasks to ease operations: Creates a ColdStart annotation to easily filter traces that have had an initialization overhead Captures any response, or full exceptions generated by the handler, and include as tracing metadata","title":"Lambda handler"},{"location":"core/tracer/#annotations-metadata","text":"Annotations are key-values associated with traces and indexed by AWS X-Ray. You can use them to filter traces and to create Trace Groups to slice and dice your transactions. Metadata are key-values also associated with traces but not indexed by AWS X-Ray. You can use them to add additional context for an operation using any native object. Annotations You can add annotations using putAnnotation method. 1 2 3 4 5 6 7 8 9 import { Tracer } from '@aws-lambda-powertools/tracer' ; const tracer = Tracer () @tracer . capture_lambda_handler const handler = ( event , context ) => { ... tracer . addAnnotation ( 'payment_response' , 'SUCCESS' ); } Metadata You can add metadata using putMetadata method. 1 2 3 4 5 6 7 8 9 10 import { Tracer } from '@aws-lambda-powertools/tracer' ; const tracer = Tracer () @tracer . capture_lambda_handler const handler = ( event , context ) => { ... const res = someLogic (); tracer . putMetadata ( 'payment_response' , res ); }","title":"Annotations &amp; Metadata"},{"location":"core/tracer/#synchronous-functions","text":"You can trace synchronous functions using the capture_method decorator. Warning When capture_response is enabled, the function response will be read and serialized as json. The serialization is performed by the aws-xray-sdk which uses the jsonpickle module. This can cause unintended consequences if there are side effects to recursively reading the returned value, for example if the decorated function response contains a file-like object or a StreamingBody for S3 objects. 1 2 3 4 5 @tracer . capture_method def collect_payment ( charge_id ): ret = requests . post ( PAYMENT_ENDPOINT ) # logic tracer . put_annotation ( \"PAYMENT_STATUS\" , \"SUCCESS\" ) # custom annotation return ret","title":"Synchronous functions"},{"location":"core/tracer/#asynchronous-and-generator-functions","text":"Warning We do not support async Lambda handler - Lambda handler itself must be synchronous You can trace asynchronous functions and generator functions (including context managers) using capture_method . Async 1 2 3 4 5 6 7 8 9 import asyncio import contextlib from aws_lambda_powertools import Tracer tracer = Tracer () @tracer . capture_method async def collect_payment (): ... Context manager 1 2 3 4 5 6 7 8 9 10 11 import asyncio import contextlib from aws_lambda_powertools import Tracer tracer = Tracer () @contextlib . contextmanager @tracer . capture_method def collect_payment_ctxman (): yield result ... Generators 1 2 3 4 5 6 7 8 9 10 import asyncio import contextlib from aws_lambda_powertools import Tracer tracer = Tracer () @tracer . capture_method def collect_payment_gen (): yield result ... The decorator will detect whether your function is asynchronous, a generator, or a context manager and adapt its behaviour accordingly. app.py 1 2 3 4 5 6 7 8 @tracer . capture_lambda_handler def handler ( evt , ctx ): asyncio . run ( collect_payment ()) with collect_payment_ctxman as result : do_something_with ( result ) another_result = list ( collect_payment_gen ())","title":"Asynchronous and generator functions"},{"location":"core/tracer/#advanced","text":"","title":"Advanced"},{"location":"core/tracer/#patching-modules","text":"Tracer automatically patches all supported libraries by X-Ray during initialization, by default. Underneath, AWS X-Ray SDK checks whether a supported library has been imported before patching. If you're looking to shave a few microseconds, or milliseconds depending on your function memory configuration, you can patch specific modules using patch_modules param: app.py 1 2 3 4 5 6 7 import boto3 import requests from aws_lambda_powertools import Tracer modules_to_be_patched = [ \"boto3\" , \"requests\" ] tracer = Tracer ( patch_modules = modules_to_be_patched )","title":"Patching modules"},{"location":"core/tracer/#disabling-response-auto-capture","text":"New in 1.9.0 Use capture_response=False parameter in both capture_lambda_handler and capture_method decorators to instruct Tracer not to serialize function responses as metadata. This is commonly useful in three scenarios You might return sensitive information you don't want it to be added to your traces You might manipulate streaming objects that can be read only once ; this prevents subsequent calls from being empty You might return more than 64K of data e.g., message too long error sensitive_data_scenario.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Tracer @tracer . capture_method ( capture_response = False ) def fetch_sensitive_information (): return \"sensitive_information\" @tracer . capture_lambda_handler ( capture_response = False ) def handler ( event , context ): sensitive_information = fetch_sensitive_information () streaming_object_scenario.py 1 2 3 4 5 6 7 from aws_lambda_powertools import Tracer @tracer . capture_method ( capture_response = False ) def get_s3_object ( bucket_name , object_key ): s3 = boto3 . client ( \"s3\" ) s3_object = get_object ( Bucket = bucket_name , Key = object_key ) return s3_object","title":"Disabling response auto-capture"},{"location":"core/tracer/#disabling-exception-auto-capture","text":"New in 1.10.0 Use capture_error=False parameter in both capture_lambda_handler and capture_method decorators to instruct Tracer not to serialize exceptions as metadata. Commonly useful in one scenario You might return sensitive information from exceptions, stack traces you might not control sensitive_data_exception.py 1 2 3 4 5 from aws_lambda_powertools import Tracer @tracer . capture_lambda_handler ( capture_error = False ) def handler ( event , context ): raise ValueError ( \"some sensitive info in the stack trace...\" )","title":"Disabling exception auto-capture"},{"location":"core/tracer/#tracing-aiohttp-requests","text":"Info This snippet assumes you have aiohttp as a dependency You can use aiohttp_trace_config function to create a valid aiohttp trace_config object . This is necessary since X-Ray utilizes aiohttp trace hooks to capture requests end-to-end. aiohttp_example.py 1 2 3 4 5 6 7 8 9 10 11 12 13 import asyncio import aiohttp from aws_lambda_powertools import Tracer from aws_lambda_powertools.tracing import aiohttp_trace_config tracer = Tracer () async def aiohttp_task (): async with aiohttp . ClientSession ( trace_configs = [ aiohttp_trace_config ()]) as session : async with session . get ( \"https://httpbin.org/json\" ) as resp : resp = await resp . json () return resp","title":"Tracing aiohttp requests"},{"location":"core/tracer/#escape-hatch-mechanism","text":"You can use tracer.provider attribute to access all methods provided by AWS X-Ray xray_recorder object. This is useful when you need a feature available in X-Ray that is not available in the Tracer utility, for example thread-safe , or context managers . escape_hatch_context_manager_example.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Tracer tracer = Tracer () @tracer . capture_lambda_handler def handler ( event , context ): with tracer . provider . in_subsegment ( '## custom subsegment' ) as subsegment : ret = some_work () subsegment . put_metadata ( 'response' , ret )","title":"Escape hatch mechanism"},{"location":"core/tracer/#concurrent-asynchronous-functions","text":"Warning As of now, X-Ray SDK will raise an exception when async functions are run and traced concurrently A safe workaround mechanism is to use in_subsegment_async available via Tracer escape hatch ( tracer.provider ). concurrent_async_workaround.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import asyncio from aws_lambda_powertools import Tracer tracer = Tracer () async def another_async_task (): async with tracer . provider . in_subsegment_async ( \"## another_async_task\" ) as subsegment : subsegment . put_annotation ( key = \"key\" , value = \"value\" ) subsegment . put_metadata ( key = \"key\" , value = \"value\" , namespace = \"namespace\" ) ... async def another_async_task_2 (): ... @tracer . capture_method async def collect_payment ( charge_id ): asyncio . gather ( another_async_task (), another_async_task_2 ()) ...","title":"Concurrent asynchronous functions"},{"location":"core/tracer/#reusing-tracer-across-your-code","text":"Tracer keeps a copy of its configuration after the first initialization. This is useful for scenarios where you want to use Tracer in more than one location across your code base. Warning When reusing Tracer in Lambda Layers, or in multiple modules, do not set auto_patch=False , because import order matters. This can result in the first Tracer config being inherited by new instances, and their modules not being patched. handler.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Tracer from payment import collect_payment tracer = Tracer ( service = \"payment\" ) @tracer . capture_lambda_handler def handler ( event , context ): charge_id = event . get ( 'charge_id' ) payment = collect_payment ( charge_id ) payment.py A new instance of Tracer will be created but will reuse the previous Tracer instance configuration, similar to a Singleton. 1 2 3 4 5 6 7 from aws_lambda_powertools import Tracer tracer = Tracer ( service = \"payment\" ) @tracer . capture_method def collect_payment ( charge_id : str ): ...","title":"Reusing Tracer across your code"},{"location":"core/tracer/#testing-your-code","text":"Tracer is disabled by default when not running in the AWS Lambda environment - This means no code changes or environment variables to be set.","title":"Testing your code"},{"location":"core/tracer/#tips","text":"Use annotations on key operations to slice and dice traces, create unique views, and create metrics from it via Trace Groups Use a namespace when adding metadata to group data more easily Annotations and metadata are added to the current subsegment opened. If you want them in a specific subsegment, use a context manager via the escape hatch mechanism","title":"Tips"}]}